---
title: "P8105_hw6_qy2234"
author: "Michael Yan"
date: "11/21/2019"
output: github_document
---

```{r setup, include=FALSE}
#### general setup
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(viridis)
library(patchwork)
library(rvest)
library(modelr)
library(mgcv)
devtools::install_github("benmarwick/wordcountaddin", type = "source", dependencies = TRUE)
knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

# problem 1
```{r}
#### load and clean the data
birthweight_data = read_csv("./data/birthweight.csv") %>%
  mutate(babysex = as.factor(babysex),
         frace = as.factor(frace),
         malform = as.factor(malform),
         mrace = as.factor(mrace))

#### check for missing data in all columns
birthweight_data %>% 
  summarise_all(funs(sum(is.na(.)))) %>% 
  knitr::kable()
```

```{r}
#### stepwise regression
fit_test = lm(bwt ~ ., data = birthweight_data)
step(fit_test, direction = 'backward')
```

* The results show that the model should include bwt ~ babysex + bhead + blength + delwt + fincome +  gaweeks + mheight + mrace + parity + ppwt + smoken. 

```{r}
#### test fitted regression model
fitted = lm(bwt ~ babysex + bhead + blength + delwt + fincome + gaweeks + mheight + mrace + parity + ppwt + smoken, data = birthweight_data)
summary(fitted)
```

* Based on the results, we see that the adjusted R-squared is 0.7173 which is resonable and the overall p-value is less than 0.05, therefore the variables used above to predict birthweight are chosen to be our predictors.

#### Plot of model residuals against fitted values 
```{r}
birthweight_data %>% 
  add_predictions(model = fitted, var = "prediction") %>% 
  add_residuals(model = fitted, var = "residuals") %>%
  ggplot(aes(x = prediction, y = residuals)) + 
  geom_point(alpha = 0.5, color = "blue") +
  geom_smooth(color = "red") + 
  labs(title = "residuals vs fitted values", 
       y = "Residuals",
       x = "Fitted Values")
```

* Based on the fitted line, which is suppose be around 0 residuals, we see that when the fitted value is low, outliers are present. In addition, outside the interval approximately from 2000-4000, the residuals are not normally distributed which is an indicator for unreliable model prediction.

```{r}
#### compare our model to two others:
set.seed(1)

#### create traing and testing dataset
crossv_mc(birthweight_data, 100) %>%
  mutate(train = map(train, as_tibble),
         test = map(test, as_tibble)) %>% 
# Fit each of three models to this dataset 
    mutate(fitted = map(train, ~lm(bwt ~ babysex + bhead + blength + gaweeks + mheight + mrace + parity + ppwt + smoken, data = .x)),
           compare_model_1 = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
           compare_model_2 = map(train, ~lm(bwt ~ babysex + blength + bhead + babysex * blength + babysex * bhead + blength * bhead + babysex * blength * bhead, data = .x))) %>% 
    mutate(rmse_model_fitted = map2_dbl(fitted, test, ~rmse(model = .x, data = .y)),
           rmse_model_1 = map2_dbl(compare_model_1, test, ~rmse(model = .x, data = .y)),
           rmse_model_2 = map2_dbl(compare_model_2, test, ~rmse(model = .x, data = .y))) %>% 
    select(starts_with("rmse")) %>% 
    gather(key = model, value = rmse) %>% 
  
#### plot the prediction error distribution for different models  
    ggplot(aes(x = model, y = rmse, fill = model)) + 
    geom_boxplot(alpha = .5, color = " red") + 
    labs(title = "Compare Fitted Model To Two Others",
       x = "Model",
       y = "RMSE")
```

* In general, we want a model that has the lowest RMSE value. In this case, our fitted model has the lowest RMSE among all three models which indicates that our model outperform the other two.


## Problem 2

```{r}
#### download weather dataset
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

```{r}
#### simple linear regression with tmax as the response and tmin as the predictor
weather_slr = lm(tmax ~ tmin, data = weather_df)
summary(weather_slr)
```
$$\hat{Y} = 7.20850 + 1.03924X_i where Y = tmax and X = tmin$$

```{r}
#### R-squared estimate and log(beta_hat_0*beta_hat_1)

# Write a function for bootstrap
bootstrap_fun = function(x){
  
  model_1 =
    x %>% 
    broom::tidy()
  
  model_2 =
    x %>%
    broom::glance()
  
  tibble(
    r_squared = pull(model_2, adj.r.squared),
    log_beta0_beta1 = log(pull(model_1, estimate)[1] * pull(model_1, estimate)[2]))
  
}
# Generate the bootstrap samples and caculate the estimates for r_squared and log_beta0_beta1
estimates = 
 weather_df %>% 
  modelr::bootstrap(n = 5000) %>% 
  mutate(
    models = map(strap, ~ lm(tmax ~ tmin, data = .x)),
    results = map(models, bootstrap_fun)) %>% 
  unnest(results) %>%
  select(-strap, -models) 

# Show the estimates of R-squared and log(beta0*beta1)
estimates[1:6,] %>%
  rename("Linear model" = `.id`) %>%
  knitr::kable(align = 'c')
```

From the table above, we can know estimates of R-squared and log(beta0*beta1).

#### *R-squared:*

#### Distribution of R-squared
```{r Distribution of R-squared}
estimates %>% 
  ggplot(aes(x = r_squared)) +
  geom_density(fill = "light blue", color = "light blue", alpha = .6, size = 1.3) +
  labs(title = "Distribution of R-squared", 
       x = "R-squared",
       y = "Density",
       caption = "Data from weather_df")
```

From the ditribution of R-squared has a little tail extending to low values, which indicates there might be some outliers included in the bootstrap samples. And it is right skewed.

#### CI for R-squared
```{r Quantile and CI for R-squared}
quantile(estimates$r_squared, c(.025, .975))
```

From above, the 95% confidence interval for R-squared is (0.893,0.927).

#### *log(beta0beta1):*

#### Distribution of log(beta0*beta1)
```{r Distribution of log(beta0beta1)}
estimates %>% 
  ggplot(aes(x = log_beta0_beta1)) +
  geom_density(fill = "light green", color = "light green", alpha = .6, size = 1.3) +
  labs(title = "Distribution of log(beta0*beta1)", 
       x = "log(beta0*beta1)",
       y = "Density",
       caption = "Data from weather_df")
```

From the distribution of log(beta0*beta1) has little extending tail on the left side and is almost symmetric, which indicates that there are almost no outliers in the bootstrap sample but the sample isn't normally distributed. And it is nearly normal distributed.

#### CI for log(beta0*beta1)
```{r Quantile and CI for log(beta0beta1)}
quantile(estimates$log_beta0_beta1, c(.025, .975))
```

